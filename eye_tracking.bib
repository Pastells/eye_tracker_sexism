
@article{hollenstein_zuco_2018,
	title = {{ZuCo}, a simultaneous {EEG} and eye-tracking resource for natural sentence reading},
	volume = {5},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018291},
	doi = {10.1038/sdata.2018.291},
	abstract = {Abstract
            
              We present the
              Zurich Cognitive Language Processing Corpus
              ({ZuCo}), a dataset combining electroencephalography ({EEG}) and eye-tracking recordings from subjects reading natural sentences. {ZuCo} includes high-density {EEG} and eye-tracking data of 12 healthy adult native English speakers, each reading natural English text for 4–6 hours. The recordings span two normal reading tasks and one task-specific reading task, resulting in a dataset that encompasses {EEG} and eye-tracking data of 21,629 words in 1107 sentences and 154,173 fixations. We believe that this dataset represents a valuable resource for natural language processing ({NLP}). The {EEG} and eye-tracking signals lend themselves to train improved machine-learning models for various tasks, in particular for information extraction tasks such as entity and relation extraction and sentiment analysis. Moreover, this dataset is useful for advancing research into the human reading and language understanding process at the level of brain activity and eye-movement.},
	pages = {180291},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Hollenstein, Nora and Rotsztejn, Jonathan and Troendle, Marius and Pedroni, Andreas and Zhang, Ce and Langer, Nicolas},
	urldate = {2025-11-20},
	date = {2018-12-11},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/KFQ7EIYA/Hollenstein et al. - 2018 - ZuCo, a simultaneous EEG and eye-tracking resource for natural sentence reading.pdf:application/pdf},
}

@misc{hollenstein_zuco_2020,
	title = {{ZuCo} 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation},
	url = {http://arxiv.org/abs/1912.00903},
	doi = {10.48550/arXiv.1912.00903},
	shorttitle = {{ZuCo} 2.0},
	abstract = {We recorded and preprocessed {ZuCo} 2.0, a new dataset of simultaneous eye-tracking and electroencephalography during natural reading and during annotation. This corpus contains gaze and brain activity data of 739 sentences, 349 in a normal reading paradigm and 390 in a task-specific paradigm, in which the 18 participants actively search for a semantic relation type in the given sentences as a linguistic annotation task. This new dataset complements {ZuCo} 1.0 by providing experiments designed to analyze the differences in cognitive processing between natural reading and annotation. The data is freely available here: https://osf.io/2urht/.},
	number = {{arXiv}:1912.00903},
	publisher = {{arXiv}},
	author = {Hollenstein, Nora and Troendle, Marius and Zhang, Ce and Langer, Nicolas},
	urldate = {2025-11-20},
	date = {2020-03-07},
	eprinttype = {arxiv},
	eprint = {1912.00903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	annotation = {Comment: Proceedings of the Language Resources and Evaluation Conference ({LREC} 2020)},
	file = {Preprint PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/HS53EE3R/Hollenstein et al. - 2020 - ZuCo 2.0 A Dataset of Physiological Recordings During Natural Reading and Annotation.pdf:application/pdf;Snapshot:/home/pol/snap/zotero-snap/common/Zotero/storage/DTMCXQAM/1912.html:text/html},
}

@inproceedings{sood_interpreting_2020,
	location = {Online},
	title = {Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension},
	url = {https://aclanthology.org/2020.conll-1.2/},
	doi = {10.18653/v1/2020.conll-1.2},
	abstract = {While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension. To this end, we introduce a novel 23 participant eye tracking dataset - {MQA}-{RC}, in which participants read movie plots and answered pre-defined questions. We compare state of the art networks based on long short-term memory ({LSTM}), convolutional neural models ({CNN}) and {XLNet} Transformer architectures. We find that higher similarity to human attention and performance significantly correlates to the {LSTM} and {CNN} models. However, we show this relationship does not hold true for the {XLNet} models – despite the fact that the {XLNet} performs best on this challenging task. Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance.},
	eventtitle = {{CoNLL} 2020},
	pages = {12--25},
	booktitle = {Proceedings of the 24th Conference on Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Sood, Ekta and Tannert, Simon and Frassinelli, Diego and Bulling, Andreas and Vu, Ngoc Thang},
	editor = {Fernández, Raquel and Linzen, Tal},
	urldate = {2025-11-20},
	date = {2020-11},
	file = {Full Text PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/UFMTR9TP/Sood et al. - 2020 - Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension.pdf:application/pdf},
}

@inproceedings{hollenstein_cognival_2019,
	location = {Hong Kong, China},
	title = {{CogniVal}: A Framework for Cognitive Word Embedding Evaluation},
	url = {https://www.aclweb.org/anthology/K19-1050},
	doi = {10.18653/v1/K19-1050},
	shorttitle = {{CogniVal}},
	abstract = {An interesting method of evaluating word representations is by how much they reﬂect the semantic representations in the human brain. However, most, if not all, previous works only focus on small datasets and a single modality. In this paper, we present the ﬁrst multimodal framework for evaluating English word representations based on cognitive lexical semantics. Six types of word embeddings are evaluated by ﬁtting them to 15 datasets of eyetracking, {EEG} and {fMRI} signals recorded during language processing. To achieve a global score over all evaluation hypotheses, we apply statistical signiﬁcance testing accounting for the multiple comparisons problem. This framework is easily extensible and available to include other intrinsic and extrinsic evaluation methods. We ﬁnd strong correlations in the results between cognitive datasets, across recording modalities and to their performance on extrinsic {NLP} tasks.},
	eventtitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning ({CoNLL})},
	pages = {538--549},
	booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Hollenstein, Nora and De La Torre, Antonio and Langer, Nicolas and Zhang, Ce},
	urldate = {2025-11-20},
	date = {2019},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/7WWBRFLJ/Hollenstein et al. - 2019 - CogniVal A Framework for Cognitive Word Embedding Evaluation.pdf:application/pdf},
}

@misc{hollenstein_relative_2021,
	title = {Relative Importance in Sentence Processing},
	url = {http://arxiv.org/abs/2106.03471},
	doi = {10.48550/arXiv.2106.03471},
	abstract = {Determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding. For human language processing, we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology. In neural language models, gradient-based saliency methods indicate the relative importance of a token for the target objective. In this work, we compare patterns of relative importance in English language processing by humans and models and analyze the underlying linguistic patterns. We find that human processing patterns in English correlate strongly with saliency-based importance in language models and not with attention-based importance. Our results indicate that saliency could be a cognitively more plausible metric for interpreting neural language models. The code is available on {GitHub}: https://github.com/beinborn/relative\_importance},
	number = {{arXiv}:2106.03471},
	publisher = {{arXiv}},
	author = {Hollenstein, Nora and Beinborn, Lisa},
	urldate = {2025-11-20},
	date = {2021-06-07},
	eprinttype = {arxiv},
	eprint = {2106.03471 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annotation = {Comment: accepted at {ACL} 2021},
	file = {Preprint PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/LU3VFSE8/Hollenstein i Beinborn - 2021 - Relative Importance in Sentence Processing.pdf:application/pdf;Snapshot:/home/pol/snap/zotero-snap/common/Zotero/storage/QQR5BILA/2106.html:text/html},
}

@inproceedings{khurana_synthesizing_2023,
	location = {Dubrovnik, Croatia},
	title = {Synthesizing Human Gaze Feedback for Improved {NLP} Performance},
	url = {https://aclanthology.org/2023.eacl-main.139},
	doi = {10.18653/v1/2023.eacl-main.139},
	abstract = {Integrating human feedback in models can improve the performance of natural language processing ({NLP}) models. Feedback can be either explicit (e.g. ranking used in training language models) or implicit (e.g. using human cognitive signals in the form of eyetracking). Prior eye tracking and {NLP} research reveal that cognitive processes, such as human scanpaths, gleaned from human gaze patterns aid in the understanding and performance of {NLP} models. However, the collection of real eyetracking data for {NLP} tasks is challenging due to the requirement of expensive and precise equipment coupled with privacy invasion issues. To address this challenge, we propose {ScanTextGAN}, a novel model for generating human scanpaths over text. We show that {ScanTextGAN}-generated scanpaths can approximate meaningful cognitive signals in human gaze patterns. We include synthetically generated scanpaths in four popular {NLP} tasks spanning six different datasets as proof of concept and show that the models augmented with generated scanpaths improve the performance of all downstream {NLP} tasks.},
	eventtitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
	pages = {1895--1908},
	booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Khurana, Varun and Kumar, Yaman and Hollenstein, Nora and Kumar, Rajesh and Krishnamurthy, Balaji},
	urldate = {2025-11-20},
	date = {2023},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/5ZVNUHND/Khurana et al. - 2023 - Synthesizing Human Gaze Feedback for Improved NLP Performance.pdf:application/pdf},
}

@article{ikhwantri_looking_2023,
	title = {Looking deep in the eyes: Investigating interpretation methods for neural models on reading tasks using human eye-movement behaviour},
	volume = {60},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457322002965},
	doi = {10.1016/j.ipm.2022.103195},
	shorttitle = {Looking deep in the eyes},
	abstract = {This paper provides the first broad overview of the relation between different interpretation methods and human eye-movement behaviour across different tasks and architectures. The interpretation methods of neural networks provide the information the machine considers important, while the human eye-gaze has been believed to be a proxy of the human cognitive process. Thus, comparing them explains machine behaviour in terms of human behaviour, leading to improvement in machine performance through minimising their difference. We consider three types of natural language processing ({NLP}) tasks: sentiment analysis, relation classification and question answering, and four interpretation methods based on: simple gradient, integrated gradient, input-perturbation and attention, and three architectures: {LSTM}, {CNN} and Transformer. We leverage two corpora annotated with eye-gaze information: the Zuco dataset and the {MQA}-{RC} dataset. This research sets up two research questions. First, we investigate whether the saliency (importance) of input-words conform with those from human eye-gaze features. To this end, we compute a saliency distance ({SD}) between input words (by an interpretation method) and an eye-gaze feature. {SD} is defined as the {KL}-divergence between the saliency distribution over input words and an eye-gaze feature. We found that the {SD} scores vary depending on the combinations of tasks, interpretation methods and architectures. Second, we investigate whether the models with good saliency conformity to human eye-gaze behaviour have better prediction performances. To this end, we propose a novel evaluation device called ‘‘{SD}-performance curve’’ ({SDPC}) which represents the cumulative model performance against the {SD} scores. {SDPC} enables us to analyse the underlying phenomena that were overlooked using only the macroscopic metrics, such as average {SD} scores and rank correlations, that are typically used in the past studies. We observe that the impact of good saliency conformity between humans and machines on task performance varies among the combinations of tasks, interpretation methods and architectures. Our findings should be considered when introducing eye-gaze information for model training to improve the model performance.},
	pages = {103195},
	number = {2},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {Ikhwantri, Fariz and Putra, Jan Wira Gotama and Yamada, Hiroaki and Tokunaga, Takenobu},
	urldate = {2025-11-20},
	date = {2023-03},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/VHRL7SUH/Ikhwantri et al. - 2023 - Looking deep in the eyes Investigating interpretation methods for neural models on reading tasks us.pdf:application/pdf},
}

@article{ikhwantri_analyzing_nodate,
	title = {Analyzing Interpretability of Summarization Model with Eye-gaze Information},
	abstract = {Interpretation methods provide saliency scores indicating the importance of input words for neural summarization models. Prior work has analyzed models by comparing them to human behavior, often using eye-gaze as a proxy for human attention in reading tasks such as classification. This paper presents a framework to analyze the model behavior in summarization by comparing it to human summarization behavior using eye-gaze data. We examine two research questions: {RQ}1) whether model saliency conforms to human gaze during summarization and {RQ}2) how model saliency and human gaze affect summarization performance. For {RQ}1, we measure conformity by calculating the correlation between model saliency and human fixation counts. For {RQ}2, we conduct ablation experiments removing words/sentences considered important by models or humans. Experiments on two datasets with human eye-gaze during summarization partially confirm that model saliency aligns with human gaze ({RQ}1). However, ablation experiments show that removing highly-attended words/sentences from the human gaze does not significantly degrade performance compared with the removal by the model saliency ({RQ}2).},
	author = {Ikhwantri, Fariz and Yamada, Hiroaki and Tokunaga, Takenobu},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/LRKBRLEC/Ikhwantri et al. - Analyzing Interpretability of Summarization Model with Eye-gaze Information.pdf:application/pdf},
}

@inproceedings{eberle_transformer_2022,
	location = {Dublin, Ireland},
	title = {Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},
	url = {https://aclanthology.org/2022.acl-long.296},
	doi = {10.18653/v1/2022.acl-long.296},
	abstract = {Learned self-attention functions in state-of-theart {NLP} models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pretrained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts. Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lowerentropy attention vectors are more faithful.},
	eventtitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {4295--4309},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Eberle, Oliver and Brandl, Stephanie and Pilot, Jonas and Søgaard, Anders},
	urldate = {2025-11-20},
	date = {2022},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/FFJ68GH4/Eberle et al. - 2022 - Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze.pdf:application/pdf},
}

@misc{ribeiro_webqamgaze_2024,
	title = {{WebQAmGaze}: A Multilingual Webcam Eye-Tracking-While-Reading Dataset},
	url = {http://arxiv.org/abs/2303.17876},
	doi = {10.48550/arXiv.2303.17876},
	shorttitle = {{WebQAmGaze}},
	abstract = {We present {WebQAmGaze}, a multilingual low-cost eye-tracking-while-reading dataset, designed as the first webcam-based eye-tracking corpus of reading to support the development of explainable computational language processing models. {WebQAmGaze} includes webcam eye-tracking data from 600 participants of a wide age range naturally reading English, German, Spanish, and Turkish texts. Each participant performs two reading tasks composed of five texts each, a normal reading and an information-seeking task, followed by a comprehension question. We compare the collected webcam data to high-quality eye-tracking recordings. The results show a moderate to strong correlation between the eye movement measures obtained with the webcam compared to those obtained with a commercial eye-tracking device. When validating the data, we find that higher fixation duration on relevant text spans accurately indicates correctness when answering the corresponding questions. This dataset advances webcam-based reading studies and opens avenues to low-cost and diverse data collection. {WebQAmGaze} is beneficial to learn about the cognitive processes behind question-answering and to apply these insights to computational models of language understanding.},
	number = {{arXiv}:2303.17876},
	publisher = {{arXiv}},
	author = {Ribeiro, Tiago and Brandl, Stephanie and Søgaard, Anders and Hollenstein, Nora},
	urldate = {2025-11-20},
	date = {2024-03-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.17876 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/SG8EU3QE/Ribeiro et al. - 2024 - WebQAmGaze A Multilingual Webcam Eye-Tracking-While-Reading Dataset.pdf:application/pdf},
}

@misc{huang_longer_2023,
	title = {Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks},
	url = {http://arxiv.org/abs/2311.00159},
	doi = {10.48550/arXiv.2311.00159},
	shorttitle = {Longer Fixations, More Computation},
	abstract = {Humans read texts at a varying pace, while machine learning models treat each token in the same way in terms of a computational process. Therefore, we ask, does it help to make models act more like humans? In this paper, we convert this intuition into a set of novel models with fixation-guided parallel {RNNs} or layers and conduct various experiments on language modeling and sentiment analysis tasks to test their effectiveness, thus providing empirical validation for this intuition. Our proposed models achieve good performance on the language modeling task, considerably surpassing the baseline model. In addition, we find that, interestingly, the fixation duration predicted by neural networks bears some resemblance to humans’ fixation. Without any explicit guidance, the model makes similar choices to humans. We also investigate the reasons for the differences between them, which explain why “model fixations” are often more suitable than human fixations, when used to guide language models.},
	number = {{arXiv}:2311.00159},
	publisher = {{arXiv}},
	author = {Huang, Xinting and Wan, Jiajing and Kritikos, Ioannis and Hollenstein, Nora},
	urldate = {2025-11-20},
	date = {2023-10-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.00159 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/WQNS9AKZ/Huang et al. - 2023 - Longer Fixations, More Computation Gaze-Guided Recurrent Neural Networks.pdf:application/pdf},
}

@article{hollenstein_entity_nodate,
	title = {Entity Recognition at First Sight: Improving {NER} with Eye Movement Information},
	abstract = {Previous research shows that eye-tracking data contains information about the lexical and syntactic properties of text, which can be used to improve natural language processing models. In this work, we leverage eye movement features from three corpora with recorded gaze information to augment a state-of-the-art neural model for named entity recognition ({NER}) with gaze embeddings. These corpora were manually annotated with named entity labels. Moreover, we show how gaze features, generalized on word type level, eliminate the need for recorded eye-tracking data at test time. The gaze-augmented models for {NER} using tokenlevel and type-level features outperform the baselines. We present the beneﬁts of eyetracking features by evaluating the {NER} models on both individual datasets as well as in cross-domain settings.},
	author = {Hollenstein, Nora and Zhang, Ce},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/RN953SBV/Hollenstein i Zhang - Entity Recognition at First Sight Improving NER with Eye Movement Information.pdf:application/pdf},
}

@inproceedings{tokyo_institute_of_technology_japan_eye-tracking_2017,
	title = {An Eye-tracking Study of Named Entity Annotation},
	isbn = {978-954-452-049-6},
	url = {http://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP097.pdf},
	doi = {10.26615/978-954-452-049-6_097},
	abstract = {Utilising effective features in machine learning-based natural language processing ({NLP}) is crucial in achieving good performance for a given {NLP} task. The paper describes a pilot study on the analysis of eye-tracking data during named entity ({NE}) annotation, aiming at obtaining insights into effective features for the {NE} recognition task. The eye gaze data were collected from 10 annotators and analysed regarding working time and ﬁxation distribution. The results of the preliminary qualitative analysis showed that human annotators tend to look at broader contexts around the target {NE} than recent state-ofthe-art automatic {NE} recognition systems and to use predicate argument relations to identify the {NE} categories.},
	eventtitle = {{RANLP} 2017 - Recent Advances in Natural Language Processing Meet Deep Learning},
	pages = {758--764},
	booktitle = {{RANLP} 2017 - Recent Advances in Natural Language Processing Meet Deep Learning},
	publisher = {Incoma Ltd. Shoumen, Bulgaria},
	author = {{Tokyo Institute of Technology, Japan} and Tokunaga, Takenobu and Nishikawa, Hitoshi and {Tokyo Institute of Technology, Japan} and Iwakura, Tomoya and {Fujitsu Laboratories LTD., Japan}},
	urldate = {2025-11-20},
	date = {2017-11-10},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/5ZKAHCMV/Tokyo Institute of Technology, Japan et al. - 2017 - An Eye-tracking Study of Named Entity Annotation.pdf:application/pdf},
}

@inproceedings{joshi_measuring_2014,
	location = {Baltimore, Maryland},
	title = {Measuring Sentiment Annotation Complexity of Text},
	url = {https://aclanthology.org/P14-2007/},
	doi = {10.3115/v1/P14-2007},
	eventtitle = {{ACL} 2014},
	pages = {36--41},
	booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Aditya and Mishra, Abhijit and Senthamilselvan, Nivvedan and Bhattacharyya, Pushpak},
	editor = {Toutanova, Kristina and Wu, Hua},
	urldate = {2025-11-20},
	date = {2014-06},
	file = {Full Text PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/EPSWRPVI/Joshi et al. - 2014 - Measuring Sentiment Annotation Complexity of Text.pdf:application/pdf},
}

@inproceedings{de_langis_comparative_2023,
	location = {Singapore},
	title = {A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models},
	url = {https://aclanthology.org/2023.conll-1.8/},
	doi = {10.18653/v1/2023.conll-1.8},
	abstract = {There is growing interest in incorporating eye-tracking data and other implicit measures of human language processing into natural language processing ({NLP}) pipelines. The data from human language processing contain unique insight into human linguistic understanding that could be exploited by language models. However, many unanswered questions remain about the nature of this data and how it can best be utilized in downstream {NLP} tasks. In this paper, we present {EyeStyliency}, an eye-tracking dataset for human processing of stylistic text (e.g., politeness). We develop an experimental protocol to collect these style-specific eye movements. We further investigate how this saliency data compares to both human annotation methods and model-based interpretability metrics. We find that while eye-tracking data is unique, it also intersects with both human annotations and model-based importance scores, providing a possible bridge between human- and machine-based perspectives. We propose utilizing this type of data to evaluate the cognitive plausibility of models that interpret style. Our eye-tracking data and processing code are publicly available.},
	eventtitle = {{CoNLL} 2023},
	pages = {108--121},
	booktitle = {Proceedings of the 27th Conference on Computational Natural Language Learning ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {de Langis, Karin and Kang, Dongyeop},
	editor = {Jiang, Jing and Reitter, David and Deng, Shumin},
	urldate = {2025-11-20},
	date = {2023-12},
	file = {Full Text PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/6S2KQB6T/de Langis i Kang - 2023 - A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Model.pdf:application/pdf},
}

@misc{hahn_modeling_2022,
	title = {Modeling Task Effects in Human Reading with Neural Network-based Attention},
	url = {http://arxiv.org/abs/1808.00054},
	doi = {10.48550/arXiv.1808.00054},
	abstract = {Research on human reading has long documented that reading behavior shows task-specific effects, but it has been challenging to build general models predicting what reading behavior humans will show in a given task. We introduce {NEAT}, a computational model of the allocation of attention in human reading, based on the hypothesis that human reading optimizes a tradeoff between economy of attention and success at a task. Our model is implemented using contemporary neural network modeling techniques, and makes explicit and testable predictions about how the allocation of attention varies across different tasks. We test this in an eyetracking study comparing two versions of a reading comprehension task, finding that our model successfully accounts for reading behavior across the tasks. Our work thus provides evidence that task effects can be modeled as optimal adaptation to task demands.},
	number = {{arXiv}:1808.00054},
	publisher = {{arXiv}},
	author = {Hahn, Michael and Keller, Frank},
	urldate = {2025-11-20},
	date = {2022-09-16},
	eprinttype = {arxiv},
	eprint = {1808.00054 [cs]},
	keywords = {Computer Science - Computation and Language},
	annotation = {Comment: Accepted by Cognition},
	file = {Preprint PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/VEIYPDC9/Hahn i Keller - 2022 - Modeling Task Effects in Human Reading with Neural Network-based Attention.pdf:application/pdf;Snapshot:/home/pol/snap/zotero-snap/common/Zotero/storage/5S785WSV/1808.html:text/html},
}

@inproceedings{tokyo_institute_of_technology_japan_eye-tracking_2017-1,
	title = {An Eye-tracking Study of Named Entity Annotation},
	isbn = {978-954-452-049-6},
	url = {http://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP097.pdf},
	doi = {10.26615/978-954-452-049-6_097},
	abstract = {Utilising effective features in machine learning-based natural language processing ({NLP}) is crucial in achieving good performance for a given {NLP} task. The paper describes a pilot study on the analysis of eye-tracking data during named entity ({NE}) annotation, aiming at obtaining insights into effective features for the {NE} recognition task. The eye gaze data were collected from 10 annotators and analysed regarding working time and ﬁxation distribution. The results of the preliminary qualitative analysis showed that human annotators tend to look at broader contexts around the target {NE} than recent state-ofthe-art automatic {NE} recognition systems and to use predicate argument relations to identify the {NE} categories.},
	eventtitle = {{RANLP} 2017 - Recent Advances in Natural Language Processing Meet Deep Learning},
	pages = {758--764},
	booktitle = {{RANLP} 2017 - Recent Advances in Natural Language Processing Meet Deep Learning},
	publisher = {Incoma Ltd. Shoumen, Bulgaria},
	author = {{Tokyo Institute of Technology, Japan} and Tokunaga, Takenobu and Nishikawa, Hitoshi and {Tokyo Institute of Technology, Japan} and Iwakura, Tomoya and {Fujitsu Laboratories LTD., Japan}},
	urldate = {2025-11-20},
	date = {2017-11-10},
	langid = {english},
	file = {PDF:/home/pol/snap/zotero-snap/common/Zotero/storage/NNLKKZHZ/Tokyo Institute of Technology, Japan et al. - 2017 - An Eye-tracking Study of Named Entity Annotation.pdf:application/pdf},
}
